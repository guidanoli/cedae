{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447d8a86",
   "metadata": {},
   "source": [
    "# Data Science applied to CEDAE data\n",
    "\n",
    "CEDAE is the coorporation that provides drinking water and wastewater services for the city of Rio de Janeiro.\n",
    "They provide plenty of data for the press and for the population, due to a law imposed by the Ministry of Health of Brazil.\n",
    "\n",
    "There is data for physical, chemical and biological parameters of the drinking water of all ETAs (Estações de Tratamento de Água, or _Water Treatment Stations_).\n",
    "In specific, the Guandu is the largest ETA of the world, providing drinking water for the municipalities of Nilópolis, Nova Iguaçu, Duque de Caxias, Belford Roxo, São João de Meriti, Itaguaí, Queimados and Rio de Janeiro.\n",
    "\n",
    "The data is in PDF form. We would like to convert this PDF into CSV so that we can vizualize it better in [Tableau](https://www.tableau.com/). First, we need to download the HTML page with the links to the PDFs. They are available in the following URL, encoded in UTF-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c2cde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "with urllib.request.urlopen('https://cedae.com.br/relatoriosguandu') as fp:\n",
    "    HTML_page = fp.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b3fb67",
   "metadata": {},
   "source": [
    "For scrapping links to PDFs relating to geosmin, we inherit from the `HTMLParser` class from the `html.parser` module. This child class finds links whose text match at least one from a list of keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5340e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html.parser\n",
    "\n",
    "class URLScrapper(html.parser.HTMLParser):\n",
    "    def __init__(self, predicate):\n",
    "        super().__init__()\n",
    "        self.URL = None\n",
    "        self.URLs = []\n",
    "        self.pred = predicate\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'a':\n",
    "            self.URL = [value.replace(' ', '%20')\n",
    "                       for name, value in attrs\n",
    "                       if name == 'href']\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if self.URL and self.pred(data):\n",
    "            self.URLs += self.URL\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'a':\n",
    "            self.URL = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd4670",
   "metadata": {},
   "source": [
    "We then feed the parser with the HTML page contents and get the scrapped URLs. We are first interested in data relating to geosmin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6592eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapper = URLScrapper(lambda data: 'GEOSMINA' in data)\n",
    "scrapper.feed(HTML_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c21b5a0",
   "metadata": {},
   "source": [
    "We create a directory for storing the PDFs (and really don't care if it already exists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b4ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    os.mkdir('data')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e3bd94",
   "metadata": {},
   "source": [
    "Finally, we download the PDFs in the recently created directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d77c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "\n",
    "for scrapped_URL in scrapper.URLs:\n",
    "    with urllib.request.urlopen(scrapped_URL) as infp:\n",
    "        parts = urllib.parse.urlparse(scrapped_URL)\n",
    "        urlpath = urllib.parse.unquote(parts.path)\n",
    "        filepath = os.path.join('data', os.path.basename(urlpath))\n",
    "        with open(filepath, 'wb') as outfp:\n",
    "            outfp.write(infp.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
