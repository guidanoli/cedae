{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "838aa00e",
   "metadata": {},
   "source": [
    "# All water supply systems (clustering)\n",
    "\n",
    "CEDAE is the coorporation that provides drinking water and wastewater services for the Rio de Janeiro State.\n",
    "They provide plenty of data regarding the quality of the water for the press and for the population, due to laws imposed by the Ministry of Health of Brazil.\n",
    "\n",
    "Here we aim to extract, compile and analyse data from all water supply systems (that we'll be denoting by the acronym *WSS*) being monitored by CEDAE. This data is also available on their webpage. The data of every WSS is routinely gathered and anually compiled into a single report.\n",
    "\n",
    "Unfortunately, this data is not presented in high granularity, as only the mean of the measurements per month are available. Although this data has been monitored and available since 2004, we have less data samples (\\~204) than the number of daily recorded samples in a single year (\\~365).\n",
    "\n",
    "The data available contains the following parameters:\n",
    "\n",
    "* Physical and Chemical\n",
    "  * Haze (*turbidez*)\n",
    "  * Aparent color\n",
    "  * Residual chlorine\n",
    "* Bacteriological\n",
    "  * Total coliforms (before and after recollection)\n",
    "  * E. coli (before and after recollection)\n",
    "\n",
    "The plan of this notebook is to do the following sequence of tasks:\n",
    "\n",
    "1. Download the HTML page where all links to PDFs reside\n",
    "2. Parse the HTML page and extract any link to PDFs and its metadata (year and WSS)\n",
    "3. Cluster similarly-named WSSs and download their PDFs in the input/ folder\n",
    "4. Cluster old reports by inspecting their PDFs and moving/renaming them to the input/ folder too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e88a13",
   "metadata": {},
   "source": [
    "## 1. Download the HTML page\n",
    "\n",
    "First we download the HTML using the `urllib.request.urlopen` method. It returns a file pointer, from which the page can be read as a stream of bytes, and decoded to UTF-8, the default string encoding for Python (and the modern internet, pretty much)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "058fb13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "page_url = 'https://cedae.com.br/relatorioanual'\n",
    "with urllib.request.urlopen(page_url) as fp:\n",
    "    page = fp.read().decode() # Read from page and decode to UTF-8 string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec30fe",
   "metadata": {},
   "source": [
    "You can check that we got indeed the HTML for the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a9cc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\r\n",
      "<html  lang=\"pt-BR\">\r\n",
      "<head id=\"Head\">\r\n",
      "<!--*******************\n"
     ]
    }
   ],
   "source": [
    "print(page[:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b7732",
   "metadata": {},
   "source": [
    "## 2. Parse the HTML page\n",
    "\n",
    "For this task, we'll be using the `HTMLParser` class from the `html.parser` module, which allows us to specify callbacks for when the parser reads the beggining tags (`<...>`), in-between text (`<a> ... </a>`), and their ending tags (`</...>`).\n",
    "\n",
    "After analysing the source code for the page we're parsing, we notice that all links that interest us are contained in tables, particularly inside `<td>` tags. Moreover, every table has a top row whose class is `thead` (probably for short for \"table head\") containing the year of the reports.\n",
    "\n",
    "For building our custom parser, we inherit the `HTMLParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "708377c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html.parser\n",
    "from datetime import datetime\n",
    "\n",
    "class MyHTMLParser(html.parser.HTMLParser):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.in_td = False\n",
    "        self.in_thead = False\n",
    "        self.year = None\n",
    "        self.link = None\n",
    "        self.links = {}\n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'td':\n",
    "            self.in_td = True\n",
    "        elif tag == 'a':\n",
    "            links = [v for k, v in attrs if k == 'href']\n",
    "            if links:\n",
    "                assert len(links) == 1, links\n",
    "                self.link = links[0]\n",
    "        elif tag == 'tr' and ('class', 'thead') in attrs:\n",
    "            self.in_thead = True\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if self.in_thead:\n",
    "            for number in [int(s) for s in data.split() if s.isdigit()]:\n",
    "                # CEDAE was created in 1975\n",
    "                if 1975 <= number <= datetime.now().year:\n",
    "                    self.year = number\n",
    "        elif self.in_td and self.link is not None:\n",
    "            assert self.year is not None\n",
    "            data = data.strip()\n",
    "            assert len(data) > 0\n",
    "            if self.year not in self.links:\n",
    "                self.links[self.year] = {}\n",
    "            assert data not in self.links[self.year], self.links[self.year][data]\n",
    "            self.links[self.year][data] = self.link\n",
    "    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'td':\n",
    "            self.in_td = False\n",
    "        elif tag == 'a':\n",
    "            self.link = None\n",
    "        elif tag == 'tr':\n",
    "            self.in_thead = False\n",
    "        elif tag == 'table':\n",
    "            self.year = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c291c05",
   "metadata": {},
   "source": [
    "We now construct a parser instance and feed it with the contents of the HTML page.\n",
    "\n",
    "The links are stored in the `links` field. It is a dictionary of dictionaries of strings. It is first indexed by the year of the reports, and second by the name of the water supply system, resulting in the link to the PDF of the annual report corresponding to that WSS in that year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fb65052",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = MyHTMLParser()\n",
    "parser.feed(page)\n",
    "links = parser.links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1f0420",
   "metadata": {},
   "source": [
    "## 3. Cluster similarly-named WSSs and download their PDFs in the input/ folder\n",
    "\n",
    "We know that some water supply systems are wrongly named. Let's create a list of all unique names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dc199af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "names = set()\n",
    "for year, reports in links.items():\n",
    "    names |= reports.keys()\n",
    "names = np.asarray(sorted(names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0594874",
   "metadata": {},
   "source": [
    "Then, we create an index that relates names to indices in this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1ac51da",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_index = {v: i for i, v in enumerate(names)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b7148",
   "metadata": {},
   "source": [
    "Now we create a list whose item `a[i]` contains the years of reports of the `i-th` string in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b925a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_reports = [{year for year, reports in links.items() if name in reports} for name in names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5944f9b6",
   "metadata": {},
   "source": [
    "Let's cluster names that don't have reports in the same year, but are very similar according to the Levenshtein distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eddaf06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Bom Jesus do Itabapoana) = (Bom Jesus)? ([Y]es/[n]o/e[x]it) \n",
      "(Cordeiro/Cantagado) = (Cordeiro)? ([Y]es/[n]o/e[x]it) \n",
      "(Coronel Teixeira/Batatal, Coronel Teixeirabatatal) = (Coronel Teixeira)? ([Y]es/[n]o/e[x]it) \n",
      "(Laranjal) = (Imunana Laranjal)? ([Y]es/[n]o/e[x]it) \n",
      "(Nossa Senhora Aparecida) = (Aparecida)? ([Y]es/[n]o/e[x]it) \n",
      "(Ourania) = (Cacaria)? ([Y]es/[n]o/e[x]it) n\n",
      "(Palmas Paulo De Frontin) = (Palmas)? ([Y]es/[n]o/e[x]it) \n",
      "(Pipeiras/Palacete) = (Pipeiras)? ([Y]es/[n]o/e[x]it) \n",
      "(São Sebastião dos Ferreiros) = (Ferreiros)? ([Y]es/[n]o/e[x]it) \n",
      "(Trajano de Morais) = (Trajano)? ([Y]es/[n]o/e[x]it) \n",
      "(Ourania) = (Osório)? ([Y]es/[n]o/e[x]it) n\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import Levenshtein\n",
    "\n",
    "nnames = len(names)\n",
    "dmatrix = np.zeros(shape=(nnames, nnames), dtype=int)  # distance matrix\n",
    "\n",
    "# Distances where user input will be requested\n",
    "# to evaluate whether two names belong in the same cluster\n",
    "min_dist = 4\n",
    "max_dist = 5\n",
    "\n",
    "for i in range(nnames):\n",
    "    for j in range(i):\n",
    "        dist = Levenshtein.distance(names[i], names[j])\n",
    "        # We evaluate first names that are substring\n",
    "        if names[i] in names[j] or names[j] in names[i]:\n",
    "            dist = min_dist\n",
    "        dmatrix[i][j] = dmatrix[j][i] = dist\n",
    "\n",
    "# matrix indices, sorted by distance (lower first)\n",
    "dmatrix_indices = sorted(((i, j) for i in range(nnames) for j in range(i)), key=lambda k: dmatrix[k[0]][k[1]])\n",
    "\n",
    "min_year = min(links.keys())\n",
    "max_year = max(links.keys())\n",
    "nyears = max_year - min_year + 1\n",
    "\n",
    "yearmap = np.zeros(shape=(nnames, nyears), dtype=bool)  # year map\n",
    "\n",
    "for i in range(nnames):\n",
    "    for year in name_reports[i]:\n",
    "        yearmap[i][year - min_year] = True\n",
    "\n",
    "clusters = np.arange(nnames)  # cluster ids (initially every name is in its own cluster)\n",
    "\n",
    "not_connected = np.zeros(shape=(nnames, nnames), dtype=bool)  # from input\n",
    "\n",
    "for i, j in dmatrix_indices:\n",
    "    dist = dmatrix[i][j]\n",
    "    if dist > max_dist:\n",
    "        break\n",
    "    ci = clusters[i]\n",
    "    cj = clusters[j]\n",
    "    yi = np.any(yearmap[clusters == ci], axis=0)\n",
    "    yj = np.any(yearmap[clusters == cj], axis=0)\n",
    "    if np.any(yi & yj):\n",
    "        continue  # cannot merge clusters\n",
    "    if np.any(not_connected[clusters == ci, clusters == cj]):\n",
    "        continue  # not merged before\n",
    "    ci_str = \", \".join(names[clusters == ci])\n",
    "    cj_str = \", \".join(names[clusters == cj])\n",
    "    if dist >= min_dist:\n",
    "        ok = input(\"({}) = ({})? ([Y]es/[n]o/e[x]it) \".format(ci_str, cj_str))\n",
    "        if 'x' in ok:\n",
    "            break\n",
    "        elif 'n' in ok:\n",
    "            not_connected[i][j] = not_connected[j][i] = 1\n",
    "            continue  # doesn't want to merge clusters\n",
    "    clusters[clusters == cj] = ci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f250390f",
   "metadata": {},
   "source": [
    "Now, we need to choose a representable sample from each cluster. We'll define a list of criterium for getting the \"best\" name. We also create a file for storing the \"codenames\" and real names of each water supply system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "372807ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def non_ascii_char_count(s):\n",
    "    return len([c for c in s if not c.isascii()])\n",
    "\n",
    "def upper_case_char_count(s):\n",
    "    return len([c for c in s if not c.isupper()])\n",
    "\n",
    "def year_of_last_report(name):\n",
    "    index = name_index[name]\n",
    "    reports = name_reports[index]\n",
    "    return max(reports)\n",
    "\n",
    "def choose_name(cluster):\n",
    "    cluster = sorted(cluster, key=year_of_last_report)\n",
    "    cluster = sorted(cluster, key=non_ascii_char_count)\n",
    "    cluster = sorted(cluster, key=upper_case_char_count)\n",
    "    cluster = sorted(cluster, key=len)\n",
    "    return cluster[-1]\n",
    "\n",
    "def convert_char(c):\n",
    "    if c.isalpha():\n",
    "        return c.lower()\n",
    "    else:\n",
    "        return '_'\n",
    "\n",
    "def format_name(name):\n",
    "    name = unidecode.unidecode(name)\n",
    "    g = map(convert_char, name)\n",
    "    return ''.join(list(g))\n",
    "\n",
    "try:\n",
    "    os.mkdir('output')\n",
    "except FileExistsError:\n",
    "    pass  # it's ok if the output folder already exists\n",
    "\n",
    "names_file = os.path.join('output', 'wss_codenames.csv')\n",
    "names_df = pd.DataFrame(columns=('name',))\n",
    "\n",
    "cluster_names = {}\n",
    "for ci in np.unique(clusters):\n",
    "    ci_names = names[clusters == ci]\n",
    "    name = choose_name(ci_names)\n",
    "    codename = format_name(name)\n",
    "    cluster_names[ci] = codename\n",
    "    names_df.loc[codename] = (name,)\n",
    "\n",
    "names_df.to_csv(names_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1181c1e1",
   "metadata": {},
   "source": [
    "Now, let's download the PDFs and store them in a nice hierarchichal directory structure composed of `input/<year>/<wss>.pdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad548059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.parse\n",
    "\n",
    "page_url_parts = urllib.parse.urlparse(page_url)\n",
    "base_url = page_url_parts._replace(path='').geturl()\n",
    "\n",
    "def normalize_url(url):\n",
    "    if url.startswith('/'):\n",
    "        url = base_url + url\n",
    "    # Heuristic: if URL has %, it must be already formatted.\n",
    "    # If not, format only the URL path\n",
    "    if '%' in url:\n",
    "        return url\n",
    "    else:\n",
    "        urlparts = url.split('/')\n",
    "        filename = urllib.parse.quote(urlparts[-1])\n",
    "        return '/'.join(urlparts[:-1] + [filename])\n",
    "\n",
    "base_path = 'input'\n",
    "for year, reports in links.items():\n",
    "    year_path = os.path.join(base_path, str(year))\n",
    "    try:\n",
    "        os.mkdir(year_path)\n",
    "    except FileExistsError:\n",
    "        pass  # it's ok if such folder already exists\n",
    "    for name, report_url in reports.items():\n",
    "        index = name_index[name]\n",
    "        cluster = clusters[index]\n",
    "        cname = cluster_names[cluster]\n",
    "        filename = cname + \".pdf\"\n",
    "        filepath = os.path.join(year_path, filename)\n",
    "        if os.path.exists(filepath) and os.path.isfile(filepath):\n",
    "            continue  # don't need to download files already local\n",
    "        report_url = normalize_url(report_url)\n",
    "        with urllib.request.urlopen(report_url) as webfp, open(filepath, 'wb') as localfp:\n",
    "            localfp.write(webfp.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f151461e",
   "metadata": {},
   "source": [
    "### 4. Cluster old reports by inspecting their PDFs and moving/renaming them to the input/ folder too\n",
    "\n",
    "Old reports are zipped and available for download in the same page as the more recent reports. For simplicity, we have already downloaded them in the `input/old` folder.\n",
    "\n",
    "In this section, we are going to inspect the textual contents of these PDFs and guess which WSS are they related to.\n",
    "\n",
    "First, let's list every report located in the `input/old` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49a38a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "old_reports = {}\n",
    "for year in os.listdir(os.path.join('input', 'old')):\n",
    "    pathname = os.path.join('input', 'old', year, '*.pdf')\n",
    "    assert year.isdigit(), \"Assumed folders are numbers\"\n",
    "    old_reports[int(year)] = list(glob.iglob(pathname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7c59cc",
   "metadata": {},
   "source": [
    "Now, for each year, we'll define regular expression patterns for obtaining the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "40989510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regex_specs(year):\n",
    "    if year < 2009:\n",
    "        return None\n",
    "#         return [(\"([Nn]?[Oo]s? )?[Mm]unicípios? d[eo] ([^,.]*?),? \"\\\n",
    "#                  \"(é|são|recebe|na região|e o distrito|somente o distrito|\"\\\n",
    "#                  \"a CEDAE|no município do Rio de Janeiro)\", 2)]\n",
    "    elif year == 2009:\n",
    "#        return [(r\"SOBRE O SISTEMA (DE )?(.*?)\\s*O MANANCIAL\", 2)]\n",
    "        return None\n",
    "    elif year == 2010:\n",
    "        return None\n",
    "#        return [(r\"SOBRE O SISTEMA (DE )?(.*?)\\s*O MANANCIAL\", 2)]\n",
    "    elif year == 2011:\n",
    "        return None\n",
    "#        return [(r\"SOBRE O SISTEMA (DE )?(.*?)\\s*o O MANANCIAL\", 2)]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "74a0d33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "eol_hyphen_patt = re.compile('-\\n')\n",
    "conj_article_patt = re.compile(' [Ee]( [OAoa])? ')\n",
    "spaces_patt = re.compile(r'\\s\\s+')\n",
    "old_names = {}\n",
    "for year, reports in old_reports.items():\n",
    "    yearlytexts = {}\n",
    "    regex_spec = get_regex_specs(year)\n",
    "    if regex_spec is None:\n",
    "        continue\n",
    "    old_names[year] = {}\n",
    "    patt_strs, groups = zip(*regex_spec)\n",
    "    patts = list(map(re.compile, patt_strs))\n",
    "    for report in reports:\n",
    "        reporttext = \"\"\n",
    "        reader = PyPDF2.PdfFileReader(report)\n",
    "        for page in reader.pages:\n",
    "            text = page.extractText()\n",
    "            text = eol_hyphen_patt.sub('', text)\n",
    "            text = spaces_patt.sub(r' ', text)\n",
    "            reporttext += text\n",
    "        names = set()\n",
    "        for patt, group in zip(patts, groups):\n",
    "            for match in patt.finditer(reporttext):\n",
    "                name = match[group]\n",
    "                name = conj_article_patt.sub('/', name)\n",
    "                names.add(name)\n",
    "        if names:\n",
    "            name = '/'.join(sorted(names))\n",
    "        else:\n",
    "            basename = os.path.basename(report)\n",
    "            name, ext = os.path.splitext(basename)\n",
    "            name = name.replace('_', ' ')\n",
    "            name = conj_article_patt.sub('/', name)\n",
    "        old_names[year][report] = name.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "61b8aaf2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2011",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-90d47e501027>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mold_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2011\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{:s} ({:s})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 2011"
     ]
    }
   ],
   "source": [
    "for path, name in old_names[2011].items():\n",
    "    print(\"{:s} ({:s})\".format(name, path.split('/')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e268048c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
