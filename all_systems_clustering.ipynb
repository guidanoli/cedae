{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "838aa00e",
   "metadata": {},
   "source": [
    "# All water supply systems (clustering)\n",
    "\n",
    "CEDAE is the coorporation that provides drinking water and wastewater services for the Rio de Janeiro State.\n",
    "They provide plenty of data regarding the quality of the water for the press and for the population, due to laws imposed by the Ministry of Health of Brazil.\n",
    "\n",
    "Here we aim to extract, compile and analyse data from all water supply systems (that we'll be denoting by the acronym *WSS*) being monitored by CEDAE. This data is also available on their webpage. The data of every WSS is routinely gathered and anually compiled into a single report.\n",
    "\n",
    "Unfortunately, this data is not presented in high granularity, as only the mean of the measurements per month are available. Although this data has been monitored and available since 2004, we have less data samples (\\~204) than the number of daily recorded samples in a single year (\\~365).\n",
    "\n",
    "The data available contains the following parameters:\n",
    "\n",
    "* Physical and Chemical\n",
    "  * Haze (*turbidez*)\n",
    "  * Aparent color\n",
    "  * Residual chlorine\n",
    "* Bacteriological\n",
    "  * Total coliforms (before and after recollection)\n",
    "  * E. coli (before and after recollection)\n",
    "\n",
    "The plan of this notebook is to do the following sequence of tasks:\n",
    "\n",
    "1. Download the HTML page where all links to PDFs reside\n",
    "2. Parse the HTML page and extract any link to PDFs and its metadata (year and WSS)\n",
    "3. Extract names of old reports from `input/old`\n",
    "4. Cluster similarly-named WSSs\n",
    "5. Download recent reports to `input/`\n",
    "6. Copy older reports from `input/old` to `input/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e88a13",
   "metadata": {},
   "source": [
    "## 1. Download the HTML page\n",
    "\n",
    "First we download the HTML using the `urllib.request.urlopen` method. It returns a file pointer, from which the page can be read as a stream of bytes, and decoded to UTF-8, the default string encoding for Python (and the modern internet, pretty much)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "058fb13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "page_url = 'https://cedae.com.br/relatorioanual'\n",
    "with urllib.request.urlopen(page_url) as fp:\n",
    "    page = fp.read().decode() # Read from page and decode to UTF-8 string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec30fe",
   "metadata": {},
   "source": [
    "You can check that we got indeed the HTML for the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a9cc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\r\n",
      "<html  lang=\"pt-BR\">\r\n",
      "<head id=\"Head\">\r\n",
      "<!--*******************\n"
     ]
    }
   ],
   "source": [
    "print(page[:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b7732",
   "metadata": {},
   "source": [
    "## 2. Parse the HTML page\n",
    "\n",
    "For this task, we'll be using the `HTMLParser` class from the `html.parser` module, which allows us to specify callbacks for when the parser reads the beggining tags (`<...>`), in-between text (`<a> ... </a>`), and their ending tags (`</...>`).\n",
    "\n",
    "After analysing the source code for the page we're parsing, we notice that all links that interest us are contained in tables, particularly inside `<td>` tags. Moreover, every table has a top row whose class is `thead` (probably for short for \"table head\") containing the year of the reports.\n",
    "\n",
    "For building our custom parser, we inherit the `HTMLParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "708377c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html.parser\n",
    "from datetime import datetime\n",
    "\n",
    "class MyHTMLParser(html.parser.HTMLParser):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.in_td = False\n",
    "        self.in_thead = False\n",
    "        self.year = None\n",
    "        self.link = None\n",
    "        self.links = {}\n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'td':\n",
    "            self.in_td = True\n",
    "        elif tag == 'a':\n",
    "            links = [v for k, v in attrs if k == 'href']\n",
    "            if links:\n",
    "                assert len(links) == 1, links\n",
    "                self.link = links[0]\n",
    "        elif tag == 'tr' and ('class', 'thead') in attrs:\n",
    "            self.in_thead = True\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if self.in_thead:\n",
    "            for number in [int(s) for s in data.split() if s.isdigit()]:\n",
    "                # CEDAE was created in 1975\n",
    "                if 1975 <= number <= datetime.now().year:\n",
    "                    self.year = number\n",
    "        elif self.in_td and self.link is not None:\n",
    "            assert self.year is not None\n",
    "            data = data.strip()\n",
    "            assert len(data) > 0\n",
    "            if self.year not in self.links:\n",
    "                self.links[self.year] = {}\n",
    "            assert data not in self.links[self.year], self.links[self.year][data]\n",
    "            self.links[self.year][data] = self.link\n",
    "    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'td':\n",
    "            self.in_td = False\n",
    "        elif tag == 'a':\n",
    "            self.link = None\n",
    "        elif tag == 'tr':\n",
    "            self.in_thead = False\n",
    "        elif tag == 'table':\n",
    "            self.year = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c291c05",
   "metadata": {},
   "source": [
    "We now construct a parser instance and feed it with the contents of the HTML page.\n",
    "\n",
    "The links are stored in the `links` field from the `MyHTMLParser` instance. It is a dictionary of dictionaries of strings. It is first indexed by the year of the reports, and second by the name of the water supply system, resulting in the link to the PDF of the annual report corresponding to that WSS in that year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fb65052",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = MyHTMLParser()\n",
    "parser.feed(page)\n",
    "yearly_named_new_reports = parser.links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d86f75",
   "metadata": {},
   "source": [
    "### 3. Extract names of old reports from `input/old`\n",
    "\n",
    "Old reports are zipped and available for download in the same page as the more recent reports.\n",
    "So that you don't have to download them yourself, they are readily available in the `input/old` folder.\n",
    "\n",
    "In this section, we are going to inspect the textual contents of these PDFs and guess which WSS are they related to.\n",
    "\n",
    "First, let's list every report located in the `input/old` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "794fd0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "yearly_old_reports = {}\n",
    "for year in os.listdir(os.path.join('input', 'old')):\n",
    "    pathname = os.path.join('input', 'old', year, '*.pdf')\n",
    "    assert year.isdigit(), \"Assumed folders are numbers\"\n",
    "    yearly_old_reports[int(year)] = list(glob.iglob(pathname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79512998",
   "metadata": {},
   "source": [
    "Now, for each year, we'll define regular expression patterns for obtaining the name (`get_regex_specs`). This function returns the regular expression pattern to match the WSS name and the group index within that pattern that contains the name.\n",
    "\n",
    "We also define some heuristics for PDFs we can't really parse (`get_report_name`). This function returns the name of the WSS guessed from the filename and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6de0e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regex_specs(year):\n",
    "    if year < 2009:\n",
    "        return [(\"([Nn]?[Oo]s? )?[Mm]unicípios? d[eo] ([^,.]*?),? \"\\\n",
    "                 \"(é|são|recebe|na região|e o distrito|somente o distrito|\"\\\n",
    "                 \"a CEDAE|no município do Rio de Janeiro)\", 2)]\n",
    "    elif 2009 <= year <= 2015:\n",
    "        return [(r\"SOBRE O SISTEMA (DE )?(.*?)\\s*o O MANANCIAL\", 2)]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_report_name(year, filename):\n",
    "    if year == 2011:\n",
    "        pass\n",
    "    elif year in (2010, 2015, 2009):\n",
    "        filename = filename.replace('_', ' ')\n",
    "    elif year == 2012:\n",
    "        filename = filename.split(' - ')[-1]\n",
    "    elif year in (2013, 2014):\n",
    "        filename = ' '.join(filename.split('_')[1:])\n",
    "    else:\n",
    "        return None\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "869fdab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "eol_hyphen_patt = re.compile('-\\n')\n",
    "conj_article_patt = re.compile(' [Ee](?: [OAoa])? ')\n",
    "spaces_patt = re.compile(r'\\s\\s+')\n",
    "\n",
    "yearly_named_old_reports = {}\n",
    "for year, old_reports in yearly_old_reports.items():\n",
    "    regex_spec = get_regex_specs(year)\n",
    "    assert regex_spec is not None, year\n",
    "    patt_strs, groups = zip(*regex_spec)\n",
    "    patts = list(map(re.compile, patt_strs))\n",
    "    yearly_named_old_reports[year] = {}\n",
    "    for report in old_reports:\n",
    "        reader = PyPDF2.PdfFileReader(report)\n",
    "        text = ' '.join([page.extractText() for page in reader.pages])\n",
    "        text = spaces_patt.sub(r' ', text)\n",
    "        text = eol_hyphen_patt.sub('', text)\n",
    "        names = set()\n",
    "        for patt, group in zip(patts, groups):\n",
    "            for match in patt.finditer(text):\n",
    "                name = match[group]\n",
    "                names.update(conj_article_patt.split(name))\n",
    "        if not names:\n",
    "            basename = os.path.basename(report)\n",
    "            filename, fileext = os.path.splitext(basename)\n",
    "            name = get_report_name(year, filename)\n",
    "            assert name is not None, (year, filename)\n",
    "            names = conj_article_patt.split(name)\n",
    "            assert names, name\n",
    "        name = '/'.join(sorted(names))\n",
    "        name = name.title()\n",
    "        yearly_named_old_reports[year][name] = report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25145bc",
   "metadata": {},
   "source": [
    "For usability, we'll combine old and new reports into one dictionary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2659213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_named_reports = {**yearly_named_old_reports, **yearly_named_new_reports}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1f0420",
   "metadata": {},
   "source": [
    "## 4. Cluster similarly-named WSSs\n",
    "\n",
    "We know that some water supply systems are wrongly named. Let's create a list of all unique names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dc199af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "names = set()\n",
    "for year, named_reports in yearly_named_reports.items():\n",
    "    names |= named_reports.keys()\n",
    "names = np.asarray(sorted(names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0594874",
   "metadata": {},
   "source": [
    "Then, we create an index that relates names to indices in this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1ac51da",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_index = {v: i for i, v in enumerate(names)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b7148",
   "metadata": {},
   "source": [
    "Now we create a list whose item `a[i]` contains the years of reports of the `i-th` string in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b925a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_reports = [{year for year, reports in yearly_named_reports.items() if name in reports} for name in names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5944f9b6",
   "metadata": {},
   "source": [
    "Let's cluster names that don't have reports in the same year, but are very similar according to the Levenshtein distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4eddaf06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Angra Dos Reis/Mangaratiba) = (Angra Dos Reis)? ([Y]es/[n]o/e[x]it) \n",
      "(Belford Roxo/Duque De Caxias/Nova Iguaçu) = (Belford Roxo)? ([Y]es/[n]o/e[x]it) \n",
      "(Bom Jesus Do Itabapoana, Bom Jesus do Itabapoana) = (Bom Jesus)? ([Y]es/[n]o/e[x]it) \n",
      "(Cachoeiras De Macacu/Tanguá) = (Cachoeiras De Macacu)? ([Y]es/[n]o/e[x]it) \n",
      "(Cardoso Moreira/Italva) = (Cardoso Moreira)? ([Y]es/[n]o/e[x]it) \n",
      "(Cordeiro) = (Cantagalo/Cordeiro)? ([Y]es/[n]o/e[x]it) \n",
      "(Cordeiro/Cantagado) = (Cantagalo/Cordeiro, Cordeiro)? ([Y]es/[n]o/e[x]it) \n",
      "(Coronel Teixeira/Batatal, Coronel Teixeirabatatal) = (Coronel Teixeira)? ([Y]es/[n]o/e[x]it) \n",
      "(Itaboraí) = (Atafona)? ([Y]es/[n]o/e[x]it) n\n",
      "(Itaguaí/Paracambi/Seropédica) = (Itaguaí, Italva)? ([Y]es/[n]o/e[x]it) \n",
      "(Japeri) = (Cacaria)? ([Y]es/[n]o/e[x]it) n\n",
      "(Japeri) = (Camorim)? ([Y]es/[n]o/e[x]it) n\n",
      "(Japeri) = (Jamapara, Jamapará)? ([Y]es/[n]o/e[x]it) n\n",
      "(Japeri/Queimados) = (Japeri)? ([Y]es/[n]o/e[x]it) \n",
      "(Japuíba) = (Japeri, Japeri/Queimados)? ([Y]es/[n]o/e[x]it) n\n",
      "(Laranjal) = (Eta Laranjal)? ([Y]es/[n]o/e[x]it) \n",
      "(Eta Laranjal, Laranjal) = (Imunana Laranjal)? ([Y]es/[n]o/e[x]it) \n",
      "(Magé) = (Anta)? ([Y]es/[n]o/e[x]it) n\n",
      "(Magé) = (Gargau, Gargaú)? ([Y]es/[n]o/e[x]it) n\n",
      "(Mesquita) = (Mantiquira, Matiquira)? ([Y]es/[n]o/e[x]it) n\n",
      "(Mesquita/Nilópolis/Rio De Janeiro/São João Do Meriti) = (Mesquita)? ([Y]es/[n]o/e[x]it) \n",
      "(Miguel Pereira/Paty Do Alferes) = (Miguel Pereira)? ([Y]es/[n]o/e[x]it) \n",
      "(Nossa Senhora Aparecida) = (Aparecida)? ([Y]es/[n]o/e[x]it) \n",
      "(Palmas Paulo De Frontin) = (Palmas)? ([Y]es/[n]o/e[x]it) \n",
      "(Pipeiras/Palacete) = (Pipeiras)? ([Y]es/[n]o/e[x]it) \n",
      "(Rio Das Ostras) = (Barra De São João/Rio Das Ostras)? ([Y]es/[n]o/e[x]it) \n",
      "(Barra De São João/Rio Das Ostras, Rio Das Ostras) = (Casemiro De Abreu (Barra De São João)/Rio Das Ostras)? ([Y]es/[n]o/e[x]it) \n",
      "(São Francisco Do Itabapoana) = (Itabapoana)? ([Y]es/[n]o/e[x]it) \n",
      "(São Gonçalo) = (Ilha De Paquetá/São Gonçalo)? ([Y]es/[n]o/e[x]it) \n",
      "(São Sebastião dos Ferreiros) = (Ferreiros)? ([Y]es/[n]o/e[x]it) \n",
      "(Trajano De Moraes, Trajano De Morais, Trajano de Morais) = (Trajano)? ([Y]es/[n]o/e[x]it) \n",
      "(Xerem, Xerém) = (Magé)? ([Y]es/[n]o/e[x]it) n\n",
      "(Barra Do Piraí) = (Barra Do Açu, Barra Do Açú, Barra do Açu)? ([Y]es/[n]o/e[x]it) n\n",
      "(Itaboraí) = (Camorim)? ([Y]es/[n]o/e[x]it) n\n",
      "(Itaboraí) = (Ipiabas)? ([Y]es/[n]o/e[x]it) n\n",
      "(Japeri, Japeri/Queimados) = (Gargau, Gargaú)? ([Y]es/[n]o/e[x]it) n\n",
      "(Juparana, Juparanã) = (Japeri, Japeri/Queimados)? ([Y]es/[n]o/e[x]it) n\n",
      "(Magé) = (Guandu, Guandú)? ([Y]es/[n]o/e[x]it) n\n",
      "(Manilha) = (Magé)? ([Y]es/[n]o/e[x]it) n\n",
      "(Mazomba) = (Magé)? ([Y]es/[n]o/e[x]it) n\n",
      "(Medanha, Mendanha) = (Magé)? ([Y]es/[n]o/e[x]it) n\n",
      "(Mesquita, Mesquita/Nilópolis/Rio De Janeiro/São João Do Meriti) = (Banquete)? ([Y]es/[n]o/e[x]it) n\n",
      "(Mesquita, Mesquita/Nilópolis/Rio De Janeiro/São João Do Meriti) = (Mantiquira, Matiquira)? ([Y]es/[n]o/e[x]it) n\n",
      "(Mesquita, Mesquita/Nilópolis/Rio De Janeiro/São João Do Meriti) = (Medanha, Mendanha)? ([Y]es/[n]o/e[x]it) n\n",
      "(Monnerat) = (Convento)? ([Y]es/[n]o/e[x]it) n\n",
      "(Muriqui) = (Mesquita, Mesquita/Nilópolis/Rio De Janeiro/São João Do Meriti)? ([Y]es/[n]o/e[x]it) n\n",
      "(Osório) = (Brasilio)? ([Y]es/[n]o/e[x]it) n\n",
      "(Osório) = (Camorim)? ([Y]es/[n]o/e[x]it) n\n",
      "(Osório) = (Japeri, Japeri/Queimados)? ([Y]es/[n]o/e[x]it) n\n",
      "(Ourania, Ourânea) = (Osório)? ([Y]es/[n]o/e[x]it) n\n",
      "(Palmas, Palmas Paulo De Frontin) = (Japeri, Japeri/Queimados)? ([Y]es/[n]o/e[x]it) n\n",
      "(Palmas, Palmas Paulo De Frontin) = (Magé)? ([Y]es/[n]o/e[x]it) n\n",
      "(Paracambi) = (Jaguarambé, Jaguarembé)? ([Y]es/[n]o/e[x]it) n\n",
      "(Paracambi) = (Marambaia)? ([Y]es/[n]o/e[x]it) n\n",
      "(Parapeuna, Parapeúna) = (Paracambi)? ([Y]es/[n]o/e[x]it) n\n",
      "(Paraíso) = (Paracambi)? ([Y]es/[n]o/e[x]it) n\n",
      "(Piabetá) = (Japeri, Japeri/Queimados)? ([Y]es/[n]o/e[x]it) n\n",
      "(Pipeiras, Pipeiras/Palacete) = (Japeri, Japeri/Queimados)? ([Y]es/[n]o/e[x]it) n\n",
      "(Pureza) = (Japeri, Japeri/Queimados)? ([Y]es/[n]o/e[x]it) n\n",
      "(Rio D'Ouro, Rio De Ouro, Rio Douro) = (Rio De Janeiro)? ([Y]es/[n]o/e[x]it) n\n",
      "(Suruí) = (Magé)? ([Y]es/[n]o/e[x]it) n\n",
      "(Tachas) = (Japeri, Japeri/Queimados)? ([Y]es/[n]o/e[x]it) n\n",
      "(Tachas) = (Magé)? ([Y]es/[n]o/e[x]it) n\n",
      "(Taylor) = (Japeri, Japeri/Queimados)? ([Y]es/[n]o/e[x]it) n\n",
      "(Taylor) = (Magé)? ([Y]es/[n]o/e[x]it) n\n",
      "(Tingua, Tinguá) = (Magé)? ([Y]es/[n]o/e[x]it) n\n",
      "(Valença) = (Japeri, Japeri/Queimados)? ([Y]es/[n]o/e[x]it) n\n",
      "(Varjão) = (Japeri, Japeri/Queimados)? ([Y]es/[n]o/e[x]it) n\n",
      "(Varjão) = (Magé)? ([Y]es/[n]o/e[x]it) n\n",
      "(Xerem, Xerém) = (Japeri, Japeri/Queimados)? ([Y]es/[n]o/e[x]it) n\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import Levenshtein\n",
    "\n",
    "nnames = len(names)\n",
    "dmatrix = np.zeros(shape=(nnames, nnames), dtype=int)  # distance matrix\n",
    "\n",
    "# Distances where user input will be requested\n",
    "# to evaluate whether two names belong in the same cluster\n",
    "min_dist = 4\n",
    "max_dist = 5\n",
    "\n",
    "for i in range(nnames):\n",
    "    for j in range(i):\n",
    "        dist = Levenshtein.distance(names[i], names[j])\n",
    "        # We evaluate first names that are substring\n",
    "        if names[i] in names[j] or names[j] in names[i]:\n",
    "            dist = min_dist\n",
    "        dmatrix[i][j] = dmatrix[j][i] = dist\n",
    "\n",
    "# matrix indices, sorted by distance (lower first)\n",
    "dmatrix_indices = sorted(((i, j) for i in range(nnames) for j in range(i)), key=lambda k: dmatrix[k[0]][k[1]])\n",
    "\n",
    "min_year = min(yearly_named_reports.keys())\n",
    "max_year = max(yearly_named_reports.keys())\n",
    "nyears = max_year - min_year + 1\n",
    "\n",
    "yearmap = np.zeros(shape=(nnames, nyears), dtype=bool)  # year map\n",
    "\n",
    "for i in range(nnames):\n",
    "    for year in name_reports[i]:\n",
    "        yearmap[i][year - min_year] = True\n",
    "\n",
    "clusters = np.arange(nnames)  # cluster ids (initially every name is in its own cluster)\n",
    "\n",
    "not_connected = np.zeros(shape=(nnames, nnames), dtype=bool)  # from input\n",
    "\n",
    "for i, j in dmatrix_indices:\n",
    "    dist = dmatrix[i][j]\n",
    "    if dist > max_dist:\n",
    "        break\n",
    "    ci = clusters[i]\n",
    "    cj = clusters[j]\n",
    "    yi = np.any(yearmap[clusters == ci], axis=0)\n",
    "    yj = np.any(yearmap[clusters == cj], axis=0)\n",
    "    if np.any(yi & yj):\n",
    "        continue  # cannot merge clusters\n",
    "    if np.any(not_connected[clusters == ci, clusters == cj]):\n",
    "        continue  # not merged before\n",
    "    ci_str = \", \".join(names[clusters == ci])\n",
    "    cj_str = \", \".join(names[clusters == cj])\n",
    "    if dist >= min_dist:\n",
    "        ok = input(\"({}) = ({})? ([Y]es/[n]o/e[x]it) \".format(ci_str, cj_str))\n",
    "        if 'x' in ok:\n",
    "            break\n",
    "        elif 'n' in ok:\n",
    "            not_connected[i][j] = not_connected[j][i] = 1\n",
    "            continue  # doesn't want to merge clusters\n",
    "    clusters[clusters == cj] = ci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f250390f",
   "metadata": {},
   "source": [
    "Now, we need to choose a representable sample from each cluster. We'll define a list of criterium for getting the \"best\" name. We also create a file for storing the \"codenames\" and real names of each water supply system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "372807ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def non_ascii_char_count(s):\n",
    "    return len([c for c in s if not c.isascii()])\n",
    "\n",
    "def upper_case_char_count(s):\n",
    "    return len([c for c in s if not c.isupper()])\n",
    "\n",
    "def year_of_last_report(name):\n",
    "    index = name_index[name]\n",
    "    reports = name_reports[index]\n",
    "    return max(reports)\n",
    "\n",
    "def choose_name(cluster):\n",
    "    cluster = sorted(cluster, key=year_of_last_report)\n",
    "    cluster = sorted(cluster, key=non_ascii_char_count)\n",
    "    cluster = sorted(cluster, key=upper_case_char_count)\n",
    "    cluster = sorted(cluster, key=len)\n",
    "    return cluster[-1]\n",
    "\n",
    "def convert_char(c):\n",
    "    if c.isalpha():\n",
    "        return c.lower()\n",
    "    else:\n",
    "        return '_'\n",
    "\n",
    "def format_name(name):\n",
    "    name = unidecode.unidecode(name)\n",
    "    g = map(convert_char, name)\n",
    "    return ''.join(list(g))\n",
    "\n",
    "try:\n",
    "    os.mkdir('output')\n",
    "except FileExistsError:\n",
    "    pass  # it's ok if the output folder already exists\n",
    "\n",
    "names_file = os.path.join('output', 'wss_codenames.csv')\n",
    "names_df = pd.DataFrame(columns=('name',))\n",
    "\n",
    "cluster_names = {}\n",
    "for ci in np.unique(clusters):\n",
    "    ci_names = names[clusters == ci]\n",
    "    name = choose_name(ci_names)\n",
    "    codename = format_name(name)\n",
    "    cluster_names[ci] = codename\n",
    "    names_df.loc[codename] = (name,)\n",
    "\n",
    "names_df.to_csv(names_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1181c1e1",
   "metadata": {},
   "source": [
    "## 5. Download recent reports to `input/`\n",
    "\n",
    "Now, let's download the PDFs and store them in a nice hierarchichal directory structure composed of `input/<year>/<wss>.pdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad548059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.parse\n",
    "\n",
    "page_url_parts = urllib.parse.urlparse(page_url)\n",
    "base_url = page_url_parts._replace(path='').geturl()\n",
    "\n",
    "def normalize_url(url):\n",
    "    if url.startswith('/'):\n",
    "        url = base_url + url\n",
    "    # Heuristic: if URL has %, it must be already formatted.\n",
    "    # If not, format only the URL path\n",
    "    if '%' in url:\n",
    "        return url\n",
    "    else:\n",
    "        urlparts = url.split('/')\n",
    "        filename = urllib.parse.quote(urlparts[-1])\n",
    "        return '/'.join(urlparts[:-1] + [filename])\n",
    "\n",
    "base_path = 'input'\n",
    "for year, reports in yearly_named_new_reports.items():\n",
    "    year_path = os.path.join(base_path, str(year))\n",
    "    try:\n",
    "        os.mkdir(year_path)\n",
    "    except FileExistsError:\n",
    "        pass  # it's ok if such folder already exists\n",
    "    for name, report_url in reports.items():\n",
    "        index = name_index[name]\n",
    "        cluster = clusters[index]\n",
    "        cname = cluster_names[cluster]\n",
    "        filename = cname + \".pdf\"\n",
    "        filepath = os.path.join(year_path, filename)\n",
    "        if os.path.exists(filepath) and os.path.isfile(filepath):\n",
    "            continue  # don't need to download files already local\n",
    "        report_url = normalize_url(report_url)\n",
    "        with urllib.request.urlopen(report_url) as webfp, open(filepath, 'wb') as localfp:\n",
    "            localfp.write(webfp.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab3b033",
   "metadata": {},
   "source": [
    "## 6. Copy older reports from `input/old` to `input/`\n",
    "\n",
    "For older reports, they are already downloaded, and in the `input/old` folder. So we only need to create folders for each year in `input/<year>` and copy them with their respective \"canonical\" names to these folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "682934cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, reports in yearly_named_old_reports.items():\n",
    "    destination_path = os.path.join('input', str(year))\n",
    "    try:\n",
    "        os.mkdir(destination_path)\n",
    "    except FileExistsError:\n",
    "        pass  # it's ok if such folder already exists\n",
    "    for name, report_path in reports.items():\n",
    "        index = name_index[name]\n",
    "        cluster = clusters[index]\n",
    "        cname = cluster_names[cluster]\n",
    "        filename = cname + \".pdf\"\n",
    "        filepath = os.path.join(destination_path, filename)\n",
    "        if os.path.exists(filepath) and os.path.isfile(filepath):\n",
    "            continue  # don't need to copy files already copied\n",
    "        with open(report_path, 'rb') as infp, open(filepath, 'wb') as outfp:\n",
    "            outfp.write(infp.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
